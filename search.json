[{"categories":null,"content":"h�e�l�l�o� �w�o�r�l�d�\r� �\n","description":"","tags":null,"title":"First","uri":"/post/first/"},{"categories":null,"content":"Clean White Theme for Hugo hello\n","description":"","tags":null,"title":"An Example Post","uri":"/post/third/"},{"categories":null,"content":"Clean White Theme for Hugo CleanWhite is a clean, elegant, but fully functional blog theme for Hugo. Here is a live demo site using this theme.\nIt is based on huxblog Jekyll Theme and Clean Blog Jekyll Theme.\nThese two upstream projects have done awesome jobs to create a blog theme, what I’m doing here is porting it to Hugo, of which I like the simplicity and the much faster compiling speed. Some other features which I think could be useful, such as site search with algolia and proxy for Disqus access in China, have also been built in the CleanWhite theme. Other fancy features of upstream projects are not supported by this Hugo theme, I’d like to make it as simple as possible and only focus on blog purpose, at least for now. While I created this theme, I followed the Hugo theme best practice and tried to make every part of the template as a replaceable partial html, so it could be much easier for you to make your customization based on it.\nScreenshots Home Post Search Disqus Wechat Pay \u0026 Alipay Quick Start Go to the directory where you have your Hugo site and run:\n$ mkdir themes\r$ cd themes\r$ git clone https://github.com/zhaohuabing/hugo-theme-cleanwhite.git If your site is already a git project, you may want to choose to add the cleanwhite theme as a submodule to avoid messing up your existing git repository.\n$ mkdir themes\r$ git submodule add https://github.com/zhaohuabing/hugo-theme-cleanwhite.git themes/hugo-theme-cleanwhite Run Hugo Build-in Server Locally\n$ hugo serve -t hugo-theme-cleanwhite Now enter localhost:1313 in the address bar of your browser.\nIf you start from scratch, there is a working Hugo site configured with the CleanWhite theme in the exampleSite directory. You can use it as a starting point for your site.\nFor more information read the official setup guide of Hugo\nConfiguration First, let’s take a look at the config.toml. It will be useful to learn how to customize your site. Feel free to play around with the settings.\nComments The optional comments system is powered by Disqus. If you want to enable comments, create an account in Disqus and write down your shortname.\n1 disqusShortname = \"your-disqus-short-name\" You can disable the comments system by leaving the disqusShortname empty.\nDisqus in China Disqus is inaccessible in China. To get it to work, we can set up a proxy with disqus-php-api in a host which sets between the client browser and the Disqus server. The idea is that if Disqus can be reached in the guest network, the blog page will show the original Disqus comments UI, otherwise, it will downgrade and use the proxy to access the Disqus, the UI will be a little different, but the visitors can still write their comments on the page.\nThe client side java script has already been integrated to CleanWhite them, but you need to set up a proxy server yourself.\nThe proxy is written in php, which can be found here: https://github.com/zhaohuabing/disqus-php-api/tree/master/api\nYou need to specify your Disqus account information in the config.php.\ndefine('PUBLIC_KEY', '');\rdefine('SECRET_KEY', '');\rdefine('DISQUS_USERNAME', '');\rdefine('DISQUS_EMAIL', '');\rdefine('DISQUS_PASSWORD', '');\rdefine('DISQUS_WEBSITE', '');\rdefine('DISQUS_SHORTNAME', ''); Set the proxy server address in the site config file of your Hugo project.\n1 disqus_proxy = \"http://yourdisqusproxy.com\" Site Search with Algolia Follow this tutorial to create your index in Algolia. The index is just the storage of the indexing data of your site in the the cloud . The search page of CleanWhite theme will utilize this indexing data to do the search.\nGo to the directory where you have your Hugo site and run the following commands:\n1 2 $ npm init $ npm install atomic-algolia --save Next, open up the newly created package.json, where we’ll add an NPM script to update your index at Algolia. Find “scripts”, and add the following:\n\"algolia\": \"atomic-algolia\" Algolia index output format has already been supported by the CleanWhite theme, so you can just build your site, then you’ll find a file called algolia.json in the root, which we can use to update your index in Algolia. Generate index file:\n1 $ hugo Create a new file in the root of your Hugo project called .env, and add the following contents:\n1 2 3 4 ALGOLIA_APP_ID={{ YOUR_APP_ID }} ALGOLIA_ADMIN_KEY={{ YOUR_ADMIN_KEY }} ALGOLIA_INDEX_NAME={{ YOUR_INDEX_NAME }} ALGOLIA_INDEX_FILE={{ PATH/TO/algolia.json }} Now you can push your index to Algolia by simply running:\n1 $ npm run algolia Add the following variables to your hugo site config so the search page can get access to algolia index data in the cloud:\nalgolia_search = true\ralgolia_appId = {{ YOUR_APP_ID }}\ralgolia_indexName = {{ YOUR_INDEX_NAME }}\ralgolia_apiKey = {{ YOUR_SEARCH_ONLY_KEY }} Open search page in your browser: http://localhost:1313/search\nAnalytics You can optionally enable Google or Baidu Analytics. Type your tracking code in the\n1 2 googleAnalytics = \"G-XXXXX\" ba_track_id = \"XXXXXXXXXXXXXXXX\" Leave the googleAnalytics or ‘ba_track_id ’ key empty to disable it.\nWechat Pay \u0026 Alipay Rewards You can enable Wechat Pay \u0026 Alipay to allow readers send you money. So if they like your articles, you may even get rewards from your writing. Now you must be motivated to write more.\nEnable Wechat Pay \u0026 Alipay in the site config 1 reward = true Replace the QR codes of Wechat Pay \u0026 Alipay by overriding the photos in folder /static/img/reward/, otherwise the money will be sent to my accounts! Thank Thanks for the great jobs of huxblog Jekyll Theme and Clean Blog Jekyll Theme which are the the two upstream projects CleanWhite Hugo theme is based on.\nFeedback If you find any problems, please feel free to raise an issue or create a pull request to fix it.\nIf it’s helpful for you, I would appreciate it if you could star this repository, thanks!\n","description":"","subtitle":"How to set up this theme","tags":null,"title":"Clean White Theme for Hugo","uri":"/post/readme/"},{"categories":["Tech"],"content":"hello world\n","description":"","tags":["tag1","tag2"],"title":"An Example Post second","uri":"/post/second/"},{"categories":["Tech"],"content":"前言 我们知道，kubernetes的Cluster Network属于私有网络，只能在cluster Network内部才能访问部署的应用，那如何才能将Kubernetes集群中的应用暴露到外部网络，为外部用户提供服务呢？本文探讨了从外部网络访问kubernetes cluster中应用的几种实现方式。\n本文尽量试着写得比较容易理解，但要做到“深入浅出”，把复杂的事情用通俗易懂的语言描述出来是非常需要功力的，个人自认尚未达到此境界，唯有不断努力。此外，kubernetes本身是一个比较复杂的系统，无法在本文中详细解释涉及的所有相关概念，否则就可能脱离了文章的主题，因此假设阅读此文之前读者对kubernetes的基本概念如docker，container，pod已有所了解。\n另外此文中的一些内容是自己的理解，由于个人的知识范围有限，可能有误，如果读者对文章中的内容有疑问或者勘误，欢迎大家指证。\nPod和Service 我们首先来了解一下Kubernetes中的Pod和Service的概念。\nPod(容器组),英文中Pod是豆荚的意思，从名字的含义可以看出，Pod是一组有依赖关系的容器，Pod包含的容器都会运行在同一个host节点上，共享相同的volumes和network namespace空间。Kubernetes以Pod为基本操作单元，可以同时启动多个相同的pod用于failover或者load balance。\nPod的生命周期是短暂的，Kubernetes根据应用的配置，会对Pod进行创建，销毁，根据监控指标进行缩扩容。kubernetes在创建Pod时可以选择集群中的任何一台空闲的Host，因此其网络地址是不固定的。由于Pod的这一特点，一般不建议直接通过Pod的地址去访问应用。\n为了解决访问Pod不方便直接访问的问题，Kubernetes采用了Service的概念，Service是对后端提供服务的一组Pod的抽象，Service会绑定到一个固定的虚拟IP上，该虚拟IP只在Kubernetes Cluster中可见，但其实该IP并不对应一个虚拟或者物理设备，而只是IPtable中的规则，然后再通过IPtable将服务请求路由到后端的Pod中。通过这种方式，可以确保服务消费者可以稳定地访问Pod提供的服务，而不用关心Pod的创建、删除、迁移等变化以及如何用一组Pod来进行负载均衡。\nService的机制如下图所示，Kube-proxy监听kubernetes master增加和删除Service以及Endpoint的消息，对于每一个Service，kube proxy创建相应的iptables规则，将发送到Service Cluster IP的流量转发到Service后端提供服务的Pod的相应端口上。 备注：虽然可以通过Service的Cluster IP和服务端口访问到后端Pod提供的服务，但该Cluster IP是Ping不通的，原因是Cluster IP只是iptable中的规则，并不对应到一个网络设备。\nService的类型 Service的类型(ServiceType)决定了Service如何对外提供服务，根据类型不同，服务可以只在Kubernetes cluster中可见，也可以暴露到Cluster外部。Service有三种类型，ClusterIP，NodePort和LoadBalancer。其中ClusterIP是Service的缺省类型，这种类型的服务会提供一个只能在Cluster内才能访问的虚拟IP，其实现机制如上面一节所述。\n通过NodePort提供外部访问入口 通过将Service的类型设置为NodePort，可以在Cluster中的主机上通过一个指定端口暴露服务。注意通过Cluster中每台主机上的该指定端口都可以访问到该服务，发送到该主机端口的请求会被kubernetes路由到提供服务的Pod上。采用这种服务类型，可以在kubernetes cluster网络外通过主机IP：端口的方式访问到服务。\n注意：官方文档中说明了Kubernetes clusterIp的流量转发到后端Pod有Iptable和kube proxy两种方式。但对Nodeport如何转发流量却语焉不详。该图来自网络，从图来看是通过kube proxy转发的，我没有去研究过源码。欢迎了解的同学跟帖说明。\n下面是通过NodePort向外暴露服务的一个例子，注意可以指定一个nodePort，也可以不指定。在不指定的情况下，kubernetes会从可用的端口范围内自动分配一个随机端口。\nkind: Service\rapiVersion: v1\rmetadata:\rname: influxdb\rspec:\rtype: NodePort\rports:\r- port: 8086\rnodePort: 30000\rselector:\rname: influxdb 通过NodePort从外部访问有下面的一些问题，自己玩玩或者进行测试时可以使用该方案，但不适宜用于生产环境。\nKubernetes cluster host的IP必须是一个well-known IP，即客户端必须知道该IP。但Cluster中的host是被作为资源池看待的，可以增加删除，每个host的IP一般也是动态分配的，因此并不能认为host IP对客户端而言是well-known IP。\n客户端访问某一个固定的host IP存在单点故障。假如一台host宕机了，kubernetes cluster会把应用 reload到另一节点上，但客户端就无法通过该host的nodeport访问应用了。\n该方案假设客户端可以访问Kubernetes host所在网络。在生产环境中，客户端和Kubernetes host网络可能是隔离的。例如客户端可能是公网中的一个手机APP，是无法直接访问host所在的私有网络的。\n因此，需要通过一个网关来将外部客户端的请求导入到Cluster中的应用中，在kubernetes中，这个网关是一个4层的load balancer。\n通过Load Balancer提供外部访问入口 通过将Service的类型设置为LoadBalancer，可以为Service创建一个外部Load Balancer。Kubernetes的文档中声明该Service类型需要云服务提供商的支持，其实这里只是在Kubernetes配置文件中提出了一个要求，即为该Service创建Load Balancer，至于如何创建则是由Google Cloud或Amazon Cloud等云服务商提供的，创建的Load Balancer不在Kubernetes Cluster的管理范围中。kubernetes 1.6版本中，WS, Azure, CloudStack, GCE and OpenStack等云提供商已经可以为Kubernetes提供Load Balancer.下面是一个Load balancer类型的Service例子：\nkind: Service\rapiVersion: v1\rmetadata:\rname: influxdb\rspec:\rtype: LoadBalancer\rports:\r- port: 8086\rselector:\rname: influxdb 部署该Service后，我们来看一下Kubernetes创建的内容\n$ kubectl get svc influxdb\rNAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE\rinfluxdb 10.97.121.42 10.13.242.236 8086:30051/TCP 39s Kubernetes首先为influxdb创建了一个集群内部可以访问的ClusterIP 10.97.121.42。由于没有指定nodeport端口，kubernetes选择了一个空闲的30051主机端口将service暴露在主机的网络上，然后通知cloud provider创建了一个load balancer，上面输出中的EEXTERNAL-IP就是load balancer的IP。\n测试使用的Cloud Provider是OpenStack，我们通过neutron lb-vip-show可以查看创建的Load Balancer详细信息。\n$ neutron lb-vip-show 9bf2a580-2ba4-4494-93fd-9b6969c55ac3\r+---------------------+--------------------------------------------------------------+\r| Field | Value |\r+---------------------+--------------------------------------------------------------+\r| address | 10.13.242.236 |\r| admin_state_up | True |\r| connection_limit | -1 |\r| description | Kubernetes external service a6ffa4dadf99711e68ea2fa163e0b082 |\r| id | 9bf2a580-2ba4-4494-93fd-9b6969c55ac3 |\r| name | a6ffa4dadf99711e68ea2fa163e0b082 |\r| pool_id | 392917a6-ed61-4924-acb2-026cd4181755 |\r| port_id | e450b80b-6da1-4b31-a008-280abdc6400b |\r| protocol | TCP |\r| protocol_port | 8086 |\r| session_persistence | |\r| status | ACTIVE |\r| status_description | |\r| subnet_id | 73f8eb91-90cf-42f4-85d0-dcff44077313 |\r| tenant_id | 4d68886fea6e45b0bc2e05cd302cccb9 |\r+---------------------+--------------------------------------------------------------+\r$ neutron lb-pool-show 392917a6-ed61-4924-acb2-026cd4181755\r+------------------------+--------------------------------------+\r| Field | Value |\r+------------------------+--------------------------------------+\r| admin_state_up | True |\r| description | |\r| health_monitors | |\r| health_monitors_status | |\r| id | 392917a6-ed61-4924-acb2-026cd4181755 |\r| lb_method | ROUND_ROBIN |\r| members | d0825cc2-46a3-43bd-af82-e9d8f1f85299 |\r| | 3f73d3bb-bc40-478d-8d0e-df05cdfb9734 |\r| name | a6ffa4dadf99711e68ea2fa163e0b082 |\r| protocol | TCP |\r| provider | haproxy |\r| status | ACTIVE |\r| status_description | |\r| subnet_id | 73f8eb91-90cf-42f4-85d0-dcff44077313 |\r| tenant_id | 4d68886fea6e45b0bc2e05cd302cccb9 |\r| vip_id | 9bf2a580-2ba4-4494-93fd-9b6969c55ac3 |\r+------------------------+--------------------------------------+\r$ neutron lb-member-list\r+--------------------------------------+--------------+---------------+--------+----------------+--------+\r| id | address | protocol_port | weight | admin_state_up | status |\r+--------------------------------------+--------------+---------------+--------+----------------+--------+\r| 3f73d3bb-bc40-478d-8d0e-df05cdfb9734 | 10.13.241.89 | 30051 | 1 | True | ACTIVE |\r| d0825cc2-46a3-43bd-af82-e9d8f1f85299 | 10.13.241.10 | 30051 | 1 | True | ACTIVE |\r+--------------------------------------+--------------+---------------+--------+----------------+-------- 可以看到OpenStack使用VIP 10.13.242.236在端口8086创建了一个Load Balancer，Load Balancer对应的Lb pool里面有两个成员10.13.241.89 和 10.13.241.10，正是Kubernetes的host节点，进入Load balancer流量被分发到这两个节点对应的Service Nodeport 30051上。\n但是如果客户端不在Openstack Neutron的私有子网上，则还需要在load balancer的VIP上关联一个floating IP，以使外部客户端可以连接到load balancer。\n部署Load balancer后，应用的拓扑结构如下图所示（注：本图假设Kubernetes Cluster部署在Openstack私有云上）。 备注：如果kubernetes环境在Public Cloud上，Loadbalancer类型的Service创建出的外部Load Balancer已经带有公网IP地址，是可以直接从外部网络进行访问的，不需要绑定floating IP这个步骤。例如在AWS上创建的Elastic Load Balancing (ELB)，有兴趣可以看一下这篇文章：Expose Services on your AWS Quick Start Kubernetes cluster。\n如果Kubernetes Cluster是在不支持LoadBalancer特性的cloud provider或者裸机上创建的，可以实现LoadBalancer类型的Service吗？应该也是可以的。Kubernetes本身并不直接支持Loadbalancer，但我们可以通过对Kubernetes进行扩展来实现，可以监听kubernetes Master的service创建消息，并根据消息部署相应的Load Balancer（如Nginx或者HAProxy），来实现Load balancer类型的Service。\n通过设置Service类型提供的是四层Load Balancer，当只需要向外暴露一个服务的时候，可以直接采用这种方式。但在一个应用需要对外提供多个服务时，采用该方式会为每一个服务（IP+Port）都创建一个外部load balancer。如下图所示 一般来说，同一个应用的多个服务/资源会放在同一个域名下，在这种情况下，创建多个Load balancer是完全没有必要的，反而带来了额外的开销和管理成本。直接将服务暴露给外部用户也会导致了前端和后端的耦合，影响了后端架构的灵活性，如果以后由于业务需求对服务进行调整会直接影响到客户端。可以通过使用Kubernetes Ingress进行L7 load balancing来解决该问题。\n采用Ingress作为七层load balancer 首先看一下引入Ingress后的应用拓扑示意图（注：本图假设Kubernetes Cluster部署在Openstack私有云上）。 这里Ingress起到了七层负载均衡器和Http方向代理的作用，可以根据不同的url把入口流量分发到不同的后端Service。外部客户端只看到foo.bar.com这个服务器，屏蔽了内部多个Service的实现方式。采用这种方式，简化了客户端的访问，并增加了后端实现和部署的灵活性，可以在不影响客户端的情况下对后端的服务部署进行调整。\n下面是Kubernetes Ingress配置文件的示例，在虚拟主机foot.bar.com下面定义了两个Path，其中/foo被分发到后端服务s1，/bar被分发到后端服务s2。\napiVersion: extensions/v1beta1\rkind: Ingress\rmetadata:\rname: test\rannotations:\ringress.kubernetes.io/rewrite-target: /\rspec:\rrules:\r- host: foo.bar.com\rhttp:\rpaths:\r- path: /foo\rbackend:\rserviceName: s1\rservicePort: 80\r- path: /bar\rbackend:\rserviceName: s2\rservicePort: 80 注意这里Ingress只描述了一个虚拟主机路径分发的要求，可以定义多个Ingress，描述不同的7层分发要求，而这些要求需要由一个Ingress Controller来实现。Ingress Contorller会监听Kubernetes Master得到Ingress的定义，并根据Ingress的定义对一个7层代理进行相应的配置，以实现Ingress定义中要求的虚拟主机和路径分发规则。Ingress Controller有多种实现，Kubernetes提供了一个基于Nginx的Ingress Controller。需要注意的是，在部署Kubernetes集群时并不会缺省部署Ingress Controller，需要我们自行部署。\n下面是部署Nginx Ingress Controller的配置文件示例，注意这里为Nginx Ingress Controller定义了一个LoadBalancer类型的Service，以为Ingress Controller提供一个外部可以访问的公网IP。\napiVersion: v1\rkind: Service\rmetadata:\rname: nginx-ingress\rspec:\rtype: LoadBalancer\rports:\r- port: 80\rname: http\r- port: 443\rname: https\rselector:\rk8s-app: nginx-ingress-lb\r---\rapiVersion: extensions/v1beta1\rkind: Deployment\rmetadata:\rname: nginx-ingress-controller\rspec:\rreplicas: 2\rrevisionHistoryLimit: 3\rtemplate:\rmetadata:\rlabels:\rk8s-app: nginx-ingress-lb\rspec:\rterminationGracePeriodSeconds: 60\rcontainers:\r- name: nginx-ingress-controller\rimage: gcr.io/google_containers/nginx-ingress-controller:0.8.3\rimagePullPolicy: Always\r//----omitted for brevity---- 备注：Google Cloud直接支持Ingress资源，如果应用部署在Google Cloud中，Google Cloud会自动为Ingress资源创建一个7层load balancer，并为之分配一个外部IP，不需要自行部署Ingress Controller。\n结论 采用Ingress加上Load balancer的方式可以将Kubernetes Cluster中的应用服务暴露给外部客户端。这种方式比较灵活，基本可以满足大部分应用的需要。但如果需要在入口处提供更强大的功能，如有更高的效率要求，需求进行安全认证，日志记录，或者需要一些应用的定制逻辑，则需要考虑采用微服务架构中的API Gateway模式，采用一个更强大的API Gateway来作为应用的流量入口。\n参考 Accessing Kubernetes Pods from Outside of the Cluster\nKubernetes nginx-ingress-controller\nUsing Kubernetes external load balancer feature\nExpose Services on your AWS Quick Start Kubernetes cluster\n","description":"我们知道，kubernetes的Cluster Network属于私有网络，只能在cluster Network内部才能访问部署的应用，那如何才能将Kubernetes集群中的应用暴露到外部网络，为外部用户提供服务呢？本文探讨了从外部网络访问kubernetes cluster中应用的几种实现方式。","tags":["Kubernetes"],"title":"如何从外部访问Kubernetes集群中的应用？","uri":"/2017/11/28/access-application-from-outside/"}]
